import json  

def chunk_json(json_data, chunk_size=1000):
    """Splits large JSON data into smaller chunks"""
    json_str = json.dumps(json_data)  
    return [json_str[i:i+chunk_size] for i in range(0, len(json_str), chunk_size)]

# Load JSON
with open("large_file.json", "r") as file:
    data = json.load(file)

chunks = chunk_json(data)
for chunk in chunks:
    response = llm.process(chunk)  # Send chunk to LLM
